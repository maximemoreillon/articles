<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>K3s on Maxime Moreillon</title><link>https://articles.maximemoreillon.com/tags/k3s/</link><description>Recent content in K3s on Maxime Moreillon</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 01 Jun 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://articles.maximemoreillon.com/tags/k3s/index.xml" rel="self" type="application/rss+xml"/><item><title>Deploying a TensorFlow model on a Jetson Nano using TensorFlow serving and K3s</title><link>https://articles.maximemoreillon.com/articles/70/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://articles.maximemoreillon.com/articles/70/</guid><description>&lt;p>The Nvidia Jetson Nano constitutes a low cost platform for AI applications, ideal for edge computing.However, due to the architecture of its CPU, deploying applications to the SBC can be challenging. In this guide, we&amp;rsquo;ll install and configure K3s, a lightweight kubernetes distribution made specifically for edge devices. Once done we&amp;rsquo;ll build and deploy an TensorFlow model in the K3s cluster.&lt;/p>
&lt;h2 id="environment-preparation">Environment preparation&lt;/h2>
&lt;p>Our objective is to deploy a TensorFlow Serving container in a Kubernetes cluster running on the Jetson Nano. Moreover, this container should fully take advantage of the CUDA capabilities of the Nano. Consequently, some preliminary environment preparation is necessary, namely .The following is based on &lt;a href="https://www.virtualthoughts.co.uk/2020/03/24/k3s-and-nvidia-jetson-nano/">this guide&lt;/a>. Additionally this article assumes that the reader has a Docker container registry available to push to and pull from.&lt;/p></description></item></channel></rss>